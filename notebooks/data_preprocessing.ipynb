{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "300465b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd05b6a",
   "metadata": {},
   "source": [
    "- read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7654c3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/output_data/telegram_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aafe5c",
   "metadata": {},
   "source": [
    "- separate metadata from message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7463ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Message\"] = df['Message'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0176c91d",
   "metadata": {},
   "source": [
    "- function to remove emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d380db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbbb3e9",
   "metadata": {},
   "source": [
    "- Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e9d37da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_amharic_punctuation(text):\n",
    "    text = text.replace('።', '.') # Replace Amharic full stop with Latin full stop\n",
    "    text = text.replace('፣', ',') # Replace Amharic comma with Latin comma\n",
    "    text = text.replace('፤', ';')\n",
    "    text = text.replace('፧', '?')\n",
    "    # You might also want to remove other specific symbols or extra spaces here\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Replace multiple spaces with single space and strip leading/trailing\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88246f",
   "metadata": {},
   "source": [
    "- Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54c23493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying cleaning steps...\n",
      "Cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying cleaning steps...\")\n",
    "df['cleaned_message'] = df['Message'].apply(remove_emojis)\n",
    "df['cleaned_message'] = df['cleaned_message'].apply(normalize_amharic_punctuation)\n",
    "print(\"Cleaning complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbfe5e6",
   "metadata": {},
   "source": [
    "- Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab36f4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abnsol/Documents/KAIM/week 4/EthioMart E-commerce Data Extractor/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (653 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete. Displaying first few rows with cleaned messages and raw tokens:\n",
      "                                             Message  \\\n",
      "0  NEW HAIR LOTION\\n\\nየፀጉር መከታ አለኝታ የሆነ ግሩም የ Tha...   \n",
      "1  በግንኙነት ጊዜ ቶሎ እየጨረሱ ተጨግረዋል❓\\nየፍቅር አጋሮን ስሜት ማርካት...   \n",
      "2  💯 vape and Hookah flavour💯\\n📌high quality Vape...   \n",
      "3  🤔የብልት መጠን ማነስ እንዲሁም በግንኙነት ወቅት ቶሎ እየረጩና እየደከሙ ...   \n",
      "4  😍ለሴቶች የሚሆን ዜና😍\\n\\nstretch mark cream (dr james...   \n",
      "\n",
      "                                     cleaned_message  \\\n",
      "0  NEW HAIR LOTION የፀጉር መከታ አለኝታ የሆነ ግሩም የ Thaila...   \n",
      "1  በግንኙነት ጊዜ ቶሎ እየጨረሱ ተጨግረዋል የፍቅር አጋሮን ስሜት ማርካት አ...   \n",
      "2  vape and Hookah flavour high quality Vape and ...   \n",
      "3  የብልት መጠን ማነስ እንዲሁም በግንኙነት ወቅት ቶሎ እየረጩና እየደከሙ አ...   \n",
      "4  ለሴቶች የሚሆን ዜና stretch mark cream (dr james) ለሁሉ...   \n",
      "\n",
      "                                          raw_tokens  \n",
      "0  [NEW, HA, ##IR, LO, ##T, ##ION, የፀጉር, መከታ, አለኝ...  \n",
      "1  [በግንኙነት, ጊዜ, ቶሎ, እየጨ, ##ረሱ, ተጨ, ##ግረዋል, የፍቅር, ...  \n",
      "2  [v, ##ape, and, H, ##ook, ##ah, fl, ##av, ##ou...  \n",
      "3  [የብልት, መጠን, ማነስ, እንዲሁም, በግንኙነት, ወቅት, ቶሎ, እየረ, ...  \n",
      "4  [ለሴቶች, የሚሆን, ዜና, str, ##et, ##ch, mar, ##k, cr...  \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# This downloads the tokenizer configuration and vocabulary files\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Davlan/bert-base-multilingual-cased-finetuned-amharic\")\n",
    "print(\"Tokenizer loaded successfully!\")\n",
    "\n",
    "# This will give you a list of subword tokens for each message\n",
    "df['raw_tokens'] = df['cleaned_message'].apply(lambda x: tokenizer.tokenize(x))\n",
    "print(\"Tokenization complete. Displaying first few rows with cleaned messages and raw tokens:\")\n",
    "print(df[['Message', 'cleaned_message', 'raw_tokens']].head()) # Display some results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bf2543",
   "metadata": {},
   "source": [
    "- save the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "963f19d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to: ../data/output_data/processed_for_labeling/processed_telegram_messages.csv\n"
     ]
    }
   ],
   "source": [
    "processed_df = df[['Id', 'cleaned_message', 'raw_tokens']].copy()\n",
    "\n",
    "processed_data_folder = os.path.join('../data', 'output_data', 'processed_for_labeling')\n",
    "os.makedirs(processed_data_folder, exist_ok=True) # Ensure the directory exists\n",
    "\n",
    "processed_csv_path = os.path.join(processed_data_folder, 'processed_telegram_messages.csv')\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "# For lists in a CSV column (like 'raw_tokens'), Pandas by default stores them as strings\n",
    "# (e.g., \"['token1', 'token2']\"). This is usually fine, you'll just parse them back later.\n",
    "processed_df.to_csv(processed_csv_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Processed data saved to: {processed_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
